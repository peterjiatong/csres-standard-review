# util.py
from pathlib import Path
import sys
from datetime import datetime
from urllib.parse import quote_plus, urljoin
import logging
import requests
from bs4 import BeautifulSoup
import time
import re
import pandas as pd
from openpyxl import load_workbook
from openpyxl.utils import get_column_letter
from docx import Document
import os
from dotenv import load_dotenv

load_dotenv()

# å¯»æ‰¾åŸºç¡€ç›®å½•
def get_base_dir():
    """Get base directory for both frozen and non-frozen states"""
    if getattr(sys, 'frozen', False):
        return Path(sys.executable).parent
    else:
        return Path(__file__).resolve().parent

# è·å–å½“å‰æ—¥æœŸå­—ç¬¦ä¸²
def get_current_date_string():
    """Get current date in MM_DD format"""
    return datetime.now().strftime("%m_%d")

# Initialize constants
BASE_DIR = get_base_dir()
MONTH_DAY = get_current_date_string()

# File paths
SRC_FILE = BASE_DIR / os.getenv("SRC")
DEST_FILE = BASE_DIR / os.getenv("DEST")

# Web scraping constants
BASE_URL = "http://www.csres.com/"
SEARCH_URL = urljoin(BASE_URL, "s.jsp")
ANTI_CRAWL_URL = "http://www.csres.com/error/noright.html"

# UAæ··æ·†
HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/125.0.0.0 Safari/537.36"),
    "Accept": ("text/html,application/xhtml+xml,application/xml;q=0.9,"
               "image/avif,image/webp,image/apng,*/*;q=0.8,"
               "application/signed-exchange;v=b3;q=0.7"),
    "Accept-Language": "zh-CN,zh;q=0.9,ja-CN;q=0.8,ja;q=0.7,en-CN;q=0.6,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",      # requests understands this
    "Referer": "http://www.csres.com/",
    "Connection": "keep-alive",
}

CURRENT = {"ç°è¡Œ", "å³å°†å®æ–½"}
# ä¸­æ–‡æ ‡ç‚¹ç¬¦å·åˆ°è‹±æ–‡æ ‡ç‚¹ç¬¦å·çš„æ˜ å°„
_ZH2EN = {
    "ï¼Œ": ",",  "ã€‚": ".",  "ï¼š": ":",  "ï¼›": ";",  "ï¼Ÿ": "?",
    "ï¼": "!",  "ï¼ˆ": "(",  "ï¼‰": ")",  "ã€": "[",  "ã€‘": "]",
    "ã€Š": "<",  "ã€‹": ">",  "â€œ": '"',  "â€": '"',  "â€˜": "'", "â€™": "'",
    "ã€": ",",  "ï¼": "-",  "ï½": "~", "ï¼‹": "+",  "ï¼…": "%",
}
# ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ç”¨äºæ›¿æ¢ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
_RE_ZH_PUNC = re.compile("|".join(map(re.escape, _ZH2EN)))

def zh_punc_to_en(text: str) -> str:
    # æŠŠ text ä¸­å¸¸è§ä¸­æ–‡å…¨è§’æ ‡ç‚¹æ›¿æ¢ä¸ºè‹±æ–‡åŠè§’æ ‡ç‚¹
    return _RE_ZH_PUNC.sub(lambda m: _ZH2EN[m.group()], text)

# æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ ‡å‡†ä»£ç 
CODE_REGEX = r"""
(?P<code>                       # â€”â€” æ•´ä½“å‘½åæ•è· "code"
    [A-Z]+                   # â‘  ä¸»å‰ç¼€  (GB / GBZ / YY123 â€¦)
    ([0-9]+)?
    (?:/[A-Z0-9]+)?             # â‘¡ å¯é€‰å­å‰ç¼€ (/T / Z1 â€¦)
    \s*                         # â‘¢ å¯é€‰ç©ºæ ¼  (GB/T  5750)
    \d+(?:\.\d+)*               # â‘£ æ•°å­—ä¸»ä½“ (5750  æˆ–  5750.1)
    (?:                         # â‘¤ **å¯é€‰** æ•°å­—èŒƒå›´
        [~\-ï½]                 #    è¿æ¥ç¬¦ï¼š~  -  æˆ–å…¨è§’ï½  
        \d+(?:\.\d+)*           #    èŒƒå›´å³ç«¯æ•°å­—  (5750.13)
    )?                          #    â† æ•´ä¸ªèŒƒå›´å—å¯çœç•¥
    (?:-\d{4})?                 # â‘¥ **å¯é€‰** å¹´ä»½åç¼€  -2019 (4ä½æ•°å­—)
    (?:[A-Z]+)?                 # â‘¦ **å¯é€‰** å°¾éƒ¨å­—æ¯  E / A / ABâ€¦
    (?:/XG\d+-\d{4})?           # â‘§ **å¯é€‰** /XG ä¿®è®¢å· /XG1-2022
)
"""

# åŸºç¡€ï¼šå·¦å³æ‹¬å·çš„å¯é€‰åŒ…è£¹
P_LEFT  = r"[ï¼ˆ(]"      # å…¨è§’ï¼ˆ æˆ– åŠè§’ (
P_RIGHT = r"[)ï¼‰]"      # åŠè§’ ) æˆ– å…¨è§’ ï¼‰

PAT_CODE = re.compile(
    rf"""^
        ({P_LEFT}\s*)?        # å¯é€‰å·¦æ‹¬å·
        {CODE_REGEX}          #   â† åŸæœ¬çš„å‘½åç»„ (?P<code>â€¦)
        (\s*{P_RIGHT})?       # å¯é€‰å³æ‹¬å·
        $""",
    re.VERBOSE
) 

PAT_NAME = re.compile(r"^ã€Š(?P<name>[^ã€‹]+)ã€‹$")   # ä¸å˜

PAT_LINE_1 = re.compile(
    rf"""{P_LEFT}?            # å¯é€‰å·¦æ‹¬å·
        \s*
        {CODE_REGEX}          # â‡’ ä»ç„¶æä¾› (?P<code>)
        \s*
        {P_RIGHT}?            # å¯é€‰å³æ‹¬å·
        \s*
        ã€Š(?P<name>[^ã€‹]+)ã€‹   #   åç§°
        .*?
    """,
    re.VERBOSE
)

PAT_LINE_2 = re.compile(
    rf"""ã€Š(?P<name>[^ã€‹]+)ã€‹  # åç§°
        .*?
        \s*
        {P_LEFT}?             # å·¦æ‹¬å·å¯é€‰
        \s*
        {CODE_REGEX}          # â‡’ (?P<code>)
        \s*
        {P_RIGHT}?            # å³æ‹¬å·å¯é€‰
    """,
    re.VERBOSE
)

def update_std_index(df_has_output):
    df_std = (
        df_has_output[["æ ‡å‡†ç¼–å·", "æ ‡å‡†åç§°", "çŠ¶æ€", "æ›¿ä»£æƒ…å†µ"]]
        .astype(str)
        .apply(lambda s: s.str.strip())
    )
    df_std["æ ‡å‡†ç¼–å·"] = df_std["æ ‡å‡†ç¼–å·"].str.replace(r"\s+", "", regex=True)
    df_std["æ ‡å‡†åç§°"] = df_std["æ ‡å‡†åç§°"].apply(zh_punc_to_en)
    return {row["æ ‡å‡†ç¼–å·"]: (row["çŠ¶æ€"], row["æ ‡å‡†åç§°"], row["æ›¿ä»£æƒ…å†µ"]) for _, row in df_std.iterrows()}

# æ ¹æ®æä¾›çš„è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
def get_path_for_log_file(path, file_name):
    """Generate unique log file path with date and index"""
    output_dir = BASE_DIR / Path(path)
    output_dir.mkdir(parents=True, exist_ok=True)
    stem = Path(file_name).stem
    suffix = Path(file_name).suffix
    glob_pat = f"{stem}_{MONTH_DAY}_*{suffix}"
    existing = sorted(output_dir.glob(glob_pat))
    next_idx = len(existing) + 1
    new_name = f"{stem}_{MONTH_DAY}_{next_idx}{suffix}"
    return output_dir / new_name

# æ ¹æ®æä¾›çš„è·¯å¾„å’Œæ–‡ä»¶åç”Ÿæˆå”¯ä¸€çš„æŠ¥å‘Šæ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¹¶åˆ›å»ºæŠ¥å‘Šæ–‡ä»¶
def get_path_for_report_folder(path_stem, file_name):
    """Generate unique report folder path and create folder"""
    parent_dir = BASE_DIR
    glob_pat = f"{path_stem}_{MONTH_DAY}_*"
    existing = sorted(d for d in parent_dir.glob(glob_pat) if d.is_dir())
    next_idx = len(existing) + 1
    new_dir = parent_dir / f"{path_stem}_{MONTH_DAY}_{next_idx}"
    new_dir.mkdir(parents=True, exist_ok=True)
    return new_dir / file_name

# è‡ªå®šä¹‰å¼‚å¸¸ç±»
class CrawlError(Exception):
    """Custom exception for web crawling errors"""
    def __init__(self, msg, code, req_headers, resp_headers):
        super().__init__(msg)
        self.code = code
        self.req_headers = req_headers
        self.resp_headers = resp_headers

# æ‰¾åˆ°è¡¨æ ¼å•å…ƒæ ¼åæå–æ–‡æœ¬
def _text_after(label, soup):
    """Extract text from table cell after finding label"""
    if label == "æ›¿ä»£æƒ…å†µ":
        # Special handling for æ›¿ä»£æƒ…å†µ
        # Look for any td containing "æ›¿ä»£æƒ…å†µ" (regardless of images)
        for td in soup.find_all("td"):
            if td.get_text(strip=True).startswith("æ›¿ä»£æƒ…å†µ"):
                next_td = td.find_next_sibling("td")
                if next_td:
                    return next_td.get_text(strip=True)
        return None
    else:
        # Original approach for other fields
        td = soup.find("td", string=re.compile(label))
        if td and td.find_next_sibling("td"):
            return td.find_next_sibling("td").get_text(strip=True)
        return None

# è®¾ç½®cookie jar
def get_jar():
    """Create cookie jar for website authentication"""
    jar = requests.cookies.RequestsCookieJar()
    jar.set("source", "www.csres.com")
    jar.set("userName", f'"{os.getenv("CSRES_USERNAME")}"')
    jar.set("userPass", os.getenv("CSRES_PASSWORD"))
    logging.info("å·²è®¾ç½®cookie jar")
    return jar

# åˆ›å»ºæœç´¢URLï¼Œæ”¯æŒä¸­æ–‡GBKç¼–ç 
def _search_url_gbk(keyword: str, page: int = 1) -> str:
    """Create search URL with GBK encoding for Chinese characters"""
    kw_gbk = quote_plus(keyword, encoding="gbk")
    return f"{SEARCH_URL}?keyword={kw_gbk}&pageNum={page}"

# çˆ¬å–å•ä¸ªæ ‡å‡†ä»£ç çš„æ•°æ®
def crawl_one_code(code, session, is_wrong_before=False):
    """Crawl data for a single standard code"""
    url_gbk = _search_url_gbk(code, page=1)

    # Retry up to 5 times for anti-crawl protection
    r_attempt = 0
    while r_attempt < 5:
        try:
            r = session.get(url_gbk, headers=HEADERS, timeout=(5, 30))
            soup = BeautifulSoup(r.text, "lxml")
            rows = soup.select('table.heng tr[bgcolor="#FFFFFF"]')
            
            # Handle known bad codes differently
            if is_wrong_before:
                if r.url != ANTI_CRAWL_URL and not rows:
                    raise CrawlError("æ— æœç´¢ç»“æœ", code, r.request.headers, r.headers)
            
            # Check for anti-crawl page and search results
            if r.url != ANTI_CRAWL_URL and rows:
                break
                
        except requests.exceptions.RequestException as e:
            logging.warning(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {r_attempt + 1}/5): {e}")
        
        r_attempt += 1
        time.sleep(2)
    
    # Handle failure after all attempts
    if r_attempt == 5:
        if r.url == ANTI_CRAWL_URL:
            raise CrawlError("ç½‘ç«™æ‹’ç»æˆ‘ä»¬è®¿é—®ï¼ˆwww.csres.com/error/noright.htmlï¼‰", 
                           code, r.request.headers, r.headers)
        raise CrawlError("æ— æœç´¢ç»“æœ", code, r.request.headers, r.headers)

    # Validate search results
    if not rows:
        raise CrawlError("æ— æœç´¢ç»“æœ", code, r.request.headers, r.headers)
    elif len(rows) > 20:
        raise CrawlError("æœç´¢ç»“æœè¿‡å¤šï¼ˆå¤§äº20ä¸ªï¼‰ï¼Œè¯·æ£€æŸ¥", code, r.request.headers, r.headers)

    # Process each result row
    hits = []
    for row in rows:
        try:
            href = row.find("a")["href"]
            status = row.find_all("td")[-1].get_text(strip=True)
            std_name = row.find_all("td")[1].get_text(strip=True)
            std_code = row.find_all("td")[0].get_text(strip=True)

            # Get detailed information from sub-page
            r2_attempt = 0
            while r2_attempt < 5:
                try:
                    r2 = session.get(urljoin(BASE_URL, href), headers=HEADERS, timeout=(5, 30))
                    soup2 = BeautifulSoup(r2.text, "lxml")
                    
                    if (r2.url != ANTI_CRAWL_URL and 
                        (_text_after("å‘å¸ƒæ—¥æœŸ", soup2) or _text_after("å®æ–½æ—¥æœŸ", soup2) or _text_after("ä½œåºŸæ—¥æœŸ", soup2))):
                        break
                        
                except requests.exceptions.RequestException as e:
                    logging.warning(f"å­é¡µé¢è¯·æ±‚å¤±è´¥ (å°è¯• {r2_attempt + 1}/5): {e}")
                
                r2_attempt += 1
                time.sleep(2)
        
            if r2_attempt == 5 and r2.url == ANTI_CRAWL_URL:
                raise CrawlError("å­é¡µé¢æ‹’ç»æˆ‘ä»¬è®¿é—®ï¼ˆwww.csres.com/error/noright.htmlï¼‰", 
                               code, r2.request.headers, r2.headers)

            # Extract detailed information
            replacement_info = ""
            if status in ("ä½œåºŸ", "åºŸæ­¢"):
                replacement_info = _text_after("æ›¿ä»£æƒ…å†µ", soup2) or ""
                # print(replacement_info)
            
            info = {
                "æ ‡å‡†ç¼–å·": std_code,
                "æ ‡å‡†åç§°": std_name,
                "çŠ¶æ€": status,
                "å‘å¸ƒæ—¥æœŸ": _text_after("å‘å¸ƒæ—¥æœŸ", soup2) or "",
                "å®æ–½æ—¥æœŸ": _text_after("å®æ–½æ—¥æœŸ", soup2) or "",
                "ä½œåºŸæ—¥æœŸ": _text_after("ä½œåºŸæ—¥æœŸ", soup2) if status in ("ä½œåºŸ", "åºŸæ­¢") else "",
                "æ›¿ä»£æƒ…å†µ": replacement_info,
            }
            hits.append((info, r2.text))
            
        except Exception as e:
            logging.warning(f"å¤„ç†è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
            continue

    return hits

# å¤„ç†å¤šä¸ªæ ‡å‡†ä»£ç çš„çˆ¬å–å’Œæ•°æ®å­˜å‚¨
def process_code(code, session, df_has_output, df_no_output_or_too_much_outputs, 
                df_date_empty, df_err, is_wrong_before=False, max_retry=9, retry_sleep=5.0):
    """Process a single code with retry logic"""
    
    for attempt in range(max_retry + 1):
        try:
            hits = crawl_one_code(code, session, is_wrong_before)
            
            for info, r2text in hits:
                # Add to results dataframe
                df_has_output.loc[len(df_has_output)] = {
                    "æ ‡å‡†ç¼–å·": info["æ ‡å‡†ç¼–å·"],
                    "æ ‡å‡†åç§°": info["æ ‡å‡†åç§°"],
                    "çŠ¶æ€": info["çŠ¶æ€"],
                    "å‘å¸ƒæ—¥æœŸ": info["å‘å¸ƒæ—¥æœŸ"],
                    "å®æ–½æ—¥æœŸ": info["å®æ–½æ—¥æœŸ"],
                    "ä½œåºŸæ—¥æœŸ": info["ä½œåºŸæ—¥æœŸ"],
                    "æ›¿ä»£æƒ…å†µ": info["æ›¿ä»£æƒ…å†µ"],
                    "ç»“æœæ·»åŠ æ—¥æœŸ": MONTH_DAY,
                }

                # Check for missing dates (debug purposes)
                if (info["å‘å¸ƒæ—¥æœŸ"] == "" and info["å®æ–½æ—¥æœŸ"] == "" and info["ä½œåºŸæ—¥æœŸ"] == ""):
                    logging.warning(f"âš ï¸  {code}: å¯èƒ½æ²¡æœ‰å‘å¸ƒæ—¥æœŸã€å®æ–½æ—¥æœŸæˆ–ä½œåºŸæ—¥æœŸ")
                    df_date_empty.loc[len(df_date_empty)] = [info["æ ‡å‡†ç¼–å·"], r2text, MONTH_DAY]

            logging.debug(f"âœ…  {code}: å…±å¤„ç†{len(hits)}ä¸ªç»“æœ")
            return

        except CrawlError as ce:
            # Handle known crawl errors
            if str(ce) in ["æ— æœç´¢ç»“æœ", "æœç´¢ç»“æœè¿‡å¤šï¼ˆå¤§äº20ä¸ªï¼‰ï¼Œè¯·æ£€æŸ¥"]:
                df_no_output_or_too_much_outputs.loc[len(df_no_output_or_too_much_outputs)] = {
                    "æ ‡å‡†ç¼–å·": ce.code,
                    "é”™è¯¯ä¿¡æ¯": str(ce),
                    "ç»“æœæ·»åŠ æ—¥æœŸ": MONTH_DAY,
                }
                logging.warning(f"âŒ  {code}: {ce}")
                return
            else:
                df_err.loc[len(df_err)] = {
                    "æ ‡å‡†ç¼–å·": ce.code,
                    "é”™è¯¯ä¿¡æ¯": str(ce),
                    "Request-Headers": dict(ce.req_headers),
                    "Response-Headers": dict(ce.resp_headers),
                    "ç»“æœæ·»åŠ æ—¥æœŸ": MONTH_DAY,
                }
                logging.error(f"âŒ  {code}: {ce}")
                return

        except Exception as e:
            # Handle unexpected errors with retry
            if attempt < max_retry:
                logging.warning(f"âš ï¸  {code}: å°è¯• {attempt + 1}/{max_retry + 1} å¤±è´¥: {e}")
                time.sleep(retry_sleep)
                continue
            else:
                # Final failure
                df_err.loc[len(df_err)] = {
                    "æ ‡å‡†ç¼–å·": code,
                    "é”™è¯¯ä¿¡æ¯": f"{e}",
                    "Request-Headers": {},
                    "Response-Headers": {},
                    "ç»“æœæ·»åŠ æ—¥æœŸ": MONTH_DAY,
                }
                logging.error(f"âŒ  {code}: {e}ï¼Œå°è¯•{max_retry + 1}æ¬¡ä»å¤±è´¥")
                return

# é…ç½®æ—¥å¿—è®°å½•
def setup_logging(file_name):
    """Setup logging configuration"""
    log_path = get_path_for_log_file("log", file_name)
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        filename=log_path,
        filemode="w",
    )

# åŠ è½½ç°æœ‰æ•°æ®
def load_existing_data():
    try:
        code_ok = pd.read_excel(SRC_FILE, sheet_name="æœ‰æœç´¢ç»“æœçš„æ ‡å‡†")["æ ‡å‡†ç¼–å·"].tolist()
        code_err = pd.read_excel(SRC_FILE, sheet_name="æ— æœç´¢ç»“æœæˆ–æœç´¢ç»“æœè¿‡å¤šçš„æ ‡å‡†")["æ ‡å‡†ç¼–å·"].tolist()
        known_codes = set(code_ok)
        # Clean whitespace from codes
        code_ok = [str(c).replace(" ", "") for c in code_ok]
        code_err = [str(c).replace(" ", "") for c in code_err]

        logging.info(f"åŸexcelä¸­â€œæœ‰æœç´¢ç»“æœçš„æ ‡å‡†â€è¡¨é•¿åº¦ä¸º:{len(code_ok)}, â€œæ— æœç´¢ç»“æœæˆ–æœç´¢ç»“æœè¿‡å¤šçš„æ ‡å‡†â€è¡¨é•¿åº¦ä¸º:{len(code_err)}")
        print(f"åŸexcelä¸­â€œæœ‰æœç´¢ç»“æœçš„æ ‡å‡†â€è¡¨é•¿åº¦ä¸º:{len(code_ok)}, â€œæ— æœç´¢ç»“æœæˆ–æœç´¢ç»“æœè¿‡å¤šçš„æ ‡å‡†â€è¡¨é•¿åº¦ä¸º:{len(code_err)}")

        return code_ok, code_err, known_codes
    
    except FileNotFoundError:
        logging.error(f"æ‰¾ä¸åˆ°æºæ–‡ä»¶: {SRC_FILE}")
        raise
    except Exception as e:
        logging.error(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise

# åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„DataFrame
def initialize_dataframes():
    """Initialize all required DataFrames"""
    df_has_output = pd.DataFrame(columns=[
        "æ ‡å‡†ç¼–å·", "æ ‡å‡†åç§°", "çŠ¶æ€", "å‘å¸ƒæ—¥æœŸ", "å®æ–½æ—¥æœŸ", "ä½œåºŸæ—¥æœŸ", "æ›¿ä»£æƒ…å†µ", "ç»“æœæ·»åŠ æ—¥æœŸ"
    ])
    df_no_output_or_too_much_outputs = pd.DataFrame(columns=[
        "æ ‡å‡†ç¼–å·", "é”™è¯¯ä¿¡æ¯", "ç»“æœæ·»åŠ æ—¥æœŸ"
    ])
    df_date_empty = pd.DataFrame(columns=[
        "æ ‡å‡†ç¼–å·", "r2.text", "ç»“æœæ·»åŠ æ—¥æœŸ"
    ])
    df_err = pd.DataFrame(columns=[
        "æ ‡å‡†ç¼–å·", "é”™è¯¯ä¿¡æ¯", "Request-Headers", "Response-Headers", "ç»“æœæ·»åŠ æ—¥æœŸ"
    ])

    logging.info("åˆå§‹åŒ–df")
    
    return df_has_output, df_no_output_or_too_much_outputs, df_date_empty, df_err

# å»é‡
def remove_duplicates(df_has_output, df_no_output_or_too_much_outputs, df_date_empty, df_err):
    """Remove duplicates from all dataframes"""    
    df_has_output = df_has_output.drop_duplicates(subset="æ ‡å‡†ç¼–å·", keep="first").reset_index(drop=True)
    df_no_output_or_too_much_outputs = df_no_output_or_too_much_outputs.drop_duplicates(subset="æ ‡å‡†ç¼–å·", keep="first").reset_index(drop=True)
    df_date_empty = df_date_empty.drop_duplicates(subset="æ ‡å‡†ç¼–å·", keep="first").reset_index(drop=True)
    df_err = df_err.drop_duplicates(subset="æ ‡å‡†ç¼–å·", keep="first").reset_index(drop=True)
    logging.info("å·²å»é‡")
    
    return df_has_output, df_no_output_or_too_much_outputs, df_date_empty, df_err

# ä¿å­˜Excelæ–‡ä»¶å¹¶è°ƒæ•´æ ¼å¼
def save_excel_with_formatting(file_path, df_has_output, df_no_output_or_too_much_outputs, df_date_empty, df_err):
    # Save to Excel
    with pd.ExcelWriter(file_path, engine="openpyxl") as writer:
        df_has_output.to_excel(writer, sheet_name="æœ‰æœç´¢ç»“æœçš„æ ‡å‡†", index=False)
        df_no_output_or_too_much_outputs.to_excel(writer, sheet_name="æ— æœç´¢ç»“æœæˆ–æœç´¢ç»“æœè¿‡å¤šçš„æ ‡å‡†", index=False)
        df_err.to_excel(writer, sheet_name="æŠ¥é”™(debugç”¨)", index=False)
        df_date_empty.to_excel(writer, sheet_name="æ ‡å‡†æ— è¯¦ç»†æ—¥æœŸ(debugç”¨)", index=False)
    
    # Adjust column widths
    wb = load_workbook(file_path)
    sheet_names = ["æœ‰æœç´¢ç»“æœçš„æ ‡å‡†", "æ— æœç´¢ç»“æœæˆ–æœç´¢ç»“æœè¿‡å¤šçš„æ ‡å‡†", "æŠ¥é”™(debugç”¨)", "æ ‡å‡†æ— è¯¦ç»†æ—¥æœŸ(debugç”¨)"]
    
    for sheet_name in sheet_names:
        if sheet_name in wb.sheetnames:
            ws = wb[sheet_name]
            for col_idx, col in enumerate(ws.iter_cols(values_only=True), 1):
                max_len = max((len(str(v)) for v in col if v is not None), default=0)
                ws.column_dimensions[get_column_letter(col_idx)].width = max(max_len + 2, 10)
    
    wb.save(file_path)

# ç”Ÿæˆæ–°å¢æ ‡å‡†æŠ¥å‘Š
def generate_new_standards_report(df_has_output, known_codes):
    """Generate report for new standards"""
    df_new = df_has_output[~df_has_output["æ ‡å‡†ç¼–å·"].isin(known_codes)]
    
    if df_new.empty:
        logging.info("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰æ–°å¢çš„æ ‡å‡†è®°å½•")
        return
    
    txt_path = get_path_for_report_folder("æ›´æ–°æ•°æ®åº“.exeçš„è¿è¡Œç»“æœ", "æ ‡å‡†æ›´æ–°æŠ¥å‘Š.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(f"æ–°å¢æ ‡å‡†ç»Ÿè®¡ï¼ˆ{MONTH_DAY}ï¼‰ å…± {len(df_new)} æ¡\n")
        f.write("-" * 78 + "\n")
        
        for _, row in df_new.iterrows():
            line = (
                f"{row['æ ‡å‡†ç¼–å·']:<20}"
                f"{row['çŠ¶æ€']:<6}"
                f"æ ‡å‡†åç§°: {row['æ ‡å‡†åç§°']}"
            )
            f.write(line + "\n")
    
    logging.info(f"ğŸ“ å·²ç”Ÿæˆæ–°å¢æ ‡å‡† TXT æŠ¥å‘Š: {txt_path}")

def generate_new_standards_report_in_exist_folder(folder, df_has_output, known_codes):
    """Generate report for new standards"""
    df_new = df_has_output[~df_has_output["æ ‡å‡†ç¼–å·"].isin(known_codes)]
    
    if df_new.empty:
        logging.info("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰æ–°å¢çš„æ ‡å‡†è®°å½•")
        return
    
    txt_path = folder / f"æ ‡å‡†æ›´æ–°æŠ¥å‘Š.txt"
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(f"æ–°å¢æ ‡å‡†ç»Ÿè®¡ï¼ˆ{MONTH_DAY}ï¼‰ å…± {len(df_new)} æ¡\n")
        f.write("-" * 78 + "\n")
        
        for _, row in df_new.iterrows():
            line = (
                f"{row['æ ‡å‡†ç¼–å·']:<20}"
                f"{row['çŠ¶æ€']:<6}"
                f"æ ‡å‡†åç§°: {row['æ ‡å‡†åç§°']}"
            )
            f.write(line + "\n")
    
    logging.info(f"ğŸ“ å·²ç”Ÿæˆæ–°å¢æ ‡å‡† TXT æŠ¥å‘Š: {txt_path}")

# Additional filter function to exclude Chinese-style codes
def is_valid_standard_code(code_text):
    """
    Filter out Chinese administrative codes and only keep alphanumeric standard codes
    """
    # Remove whitespace for checking
    clean_code = code_text.replace(" ", "")
    
    # Exclude patterns that contain Chinese characters or administrative terms
    chinese_patterns = [
        r'[\u4e00-\u9fff]',  # Any Chinese characters
        r'ä¸»å¸­ä»¤',            # Presidential decree
        r'å›½åŠ¡é™¢ä»¤',          # State Council decree  
        r'å·',               # Number (Chinese)
        r'ç¬¬\d+å·',          # "No. X" pattern
        r'ã€”\d+ã€•',          # Bracket notation like ã€”2024ã€•
    ]
    
    for pattern in chinese_patterns:
        if re.search(pattern, code_text):
            return False
    
    # Must start with letters (standard prefixes)
    if not re.match(r'^[A-Z]+', clean_code):
        return False
        
    # Must contain numbers (all standards have numbers)
    if not re.search(r'\d', clean_code):
        return False
    
    return True

def extract_from_docx(path: Path):
    doc = Document(path)
    results = []

    def feed_text(text: str):
        # å»é™¤ç©ºæ ¼
        text = text.strip()
        if not text:
            return False
        found = False        
        for pat in (PAT_LINE_1, PAT_LINE_2):
            for m in pat.finditer(text):
                orig_code = m.group("code")
                if not is_valid_standard_code(orig_code):
                    continue
                code = orig_code.replace(" ", "")
                name = zh_punc_to_en(m.group("name").strip())
                results.append((orig_code, code, name))
                found = True
        return found

    # æ®µè½
    prev = ""
    check = False
    for i in doc.paragraphs:
        p = i.text.replace('\r','\n').split('\n')
        for line in p:
            cur = line
            if check:
                feed_text(prev + cur)
                check = False
                continue
            found = feed_text(line)
            if not found:
                prev = cur               # shift look-back
                check = True
    # è¡¨æ ¼
    for tbl in doc.tables:
        for row in tbl.rows:
            cells = [c.text.strip() for c in row.cells if c.text.strip()]
            if not cells:
                continue

            # è¡Œçº§â€œä¸€å¯¹â€åŒ¹é…
            codes = [c for c in cells if PAT_CODE.fullmatch(c.replace(" ", ""))]
            names = [m.group("name") for m in map(PAT_NAME.fullmatch, cells) if m]
            if len(codes) == 1 and len(names) == 1:
                results.append((codes[0], codes[0].replace(" ", ""), zh_punc_to_en(names[0])))
                continue

            # å¦åˆ™é€è¡Œé€æ ¼åŒ¹é…
            for cell in cells:
                for line in cell.splitlines():
                    feed_text(line)
    return results

def normalize_name(s: str) -> str:
    """æ¯”è¾ƒå‰ï¼Œå¯¹åç§°å†åšä¸€æ¬¡ç»Ÿä¸€ï¼šå»ç©ºæ ¼ + åŠè§’åŒ–"""
    return zh_punc_to_en(s).replace(" ", "")
